---
title: 极大似然法和最小二乘法
date: 2018-04-20 11:48:09
tags:
- Math
- Algorithm
category:
- Math
- Algorithm
mathjax: true
---

# 相对熵（又称KL散度，Kullback-Leibler divergence）
它是描述两个概率分布P和Q差异的一种方法。他是***非对称的***，$D(P||Q) \ne D(Q||P)$

设$P(x)$和$Q(x)$是X取值的两个离散概率分布，则P对Q的相对熵为:
$$D(P||Q)=\sum P(x)\log{\frac{P(x)}{Q(x)}}$$
对于连续的随机变量，定义为：
$$D(P||Q)=\int P(x)\log{\frac{P(x)}{Q(x)}}d(x)$$


# 最大似然估计（MLE，maximum likelihood estimation）

## 前提条件
**所有采样都是独立同分布的**

## 核心思想
**已经发生的事就是最有可能发生的事**
现在已经拿到了很多个样本（你的数据集中所有因变量），这些样本值已经实现，最大似然估计就是去找到那个（组）参数估计值，使得前面已经实现的样本值发生概率最大。因为你手头上的样本已经实现了，其发生概率最大才符合逻辑。这时是求样本所有观测的联合概率最大化（度量的是KL散度），是个连乘积，只要取对数，就变成了线性加总。此时通过对参数求导数，并令一阶导数为零，就可以通过解方程（组），得到最大似然估计值。

## 求解步骤
1. 构建似然函数
{% math %}$argmaxL(\theta)=argmax\prod_{i=1}^np(x_i;\theta)${% endmath %}
2. 两边取对数
{% math %}$argmax ln L(\theta)=argmax \sum_{i=1}^n ln p(x_i;\theta)${% endmath %}
3. 对$lnL(\theta)$求导并令其等于0
{% math %}$\frac{d ln L(\theta)}{d\theta}=0${% endmath %}


# 最小二乘法(OLS, ordinary least square estimator)

## 核心思想
找到一个（组）估计值，使得实际值与估计值的距离最小。本来用两者差的绝对值汇总并使之最小是最理想的，但绝对值在数学上求最小值比较麻烦，因而替代做法是，找一个（组）估计值，使得实际值与估计值之差的平方加总之后的值最小（度量的是L2 norm distance），称为最小二乘。这时，将这个差的平方的和式对参数求导数，并取一阶导数为零，就是OLSE（ordinary least square estimator）。


***误差服从高斯分布的情况下，最小二乘法等价于极大似然估计，因此可以认为最小二乘法是最大似然估计在高斯分布下的一个推导。***


# 最大后验概率估计（MAP, maximum a posteriori estimation）

## 核心思想
最大似然估计是求参数$\theta$，使似然函数$P(x_0|\theta)$最大，但并没有考虑先验概率$P(\theta)$。最大后延概率估计则是想求$\theta$使$P(x_0|\theta)P(\theta)$最大。即求得的$\theta$不单单让似然函数大，$\theta$自己出现的先验概率也大。
MAP其实是在最大化：（因为$P(x_0)$是已知的，确定的）
{% math %}$P(\theta|x_0)=\frac{P(x_0|\theta)P(\theta)}{P(x_0)}${% endmath %}

## 求解步骤
1. 构建函数
{% math %}$arg max L(\theta)=argmax\prod_{i=1}^np(x_i;\theta)p(\theta)=argmax\prod_{i=1}^np(\theta;x_i)${% endmath %}
2. 后续步骤同MLE









