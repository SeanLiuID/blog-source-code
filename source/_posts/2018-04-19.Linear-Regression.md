---
title: Linear_Regression
date: 2018-04-19 10:01:48
tags:
- Machine learning
- Algorithm
category:
- Machine learning
- Algorithm
mathjax: true
---

## 基本形式
$$f(x)=\omega^Tx+b$$
线性回归试图学得 $f(x_i)=\omega^Tx_i+b$ ,使得 $f(x_i)\approx y_i$ 

## 一元线性回归
使用最小二乘法（least square method）进行参数估计（parameter estimation）
{% math %}$(\omega^*,b^*)=arg min\sum_{i=1}^m{(f(x_i)-y_i)^2}=arg min\sum_{i=1}^m{(y_i-\omega x_i-b)^2}${% endmath %}

基于上式对$\omega和b$求偏导，并令其为0，可得到$\omega和b$的最有闭式解

## 多元线性回归（multivariate linear regression）
更一般的，样本有多个属性描述（多个特征）
简化一下公式，可以把$\omega和b$吸收入向量形式$\hat{\omega}=(\omega;b)$，把数据集D表示成为一个m*（d+1）矩阵
{% math %}X=\begin{bmatrix}
{x_{11}}&{x_{12}}&{\cdots}&{x_{1n}}&{1}\\
{x_{21}}&{x_{22}}&{\cdots}&{x_{2n}}&{1}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}&{\vdots}\\
{x_{m1}}&{x_{m2}}&{\cdots}&{x_{mn}}&{1}\\
\end{bmatrix}{% endmath %}

此时，$f(x_i)=\omega^Tx_i+b_i=X\hat{\omega}$
{% math %}\hat{\omega}^*=argmin(y-X\hat{\omega})^T(y-X\hat{\omega}){% endmath %}

## 广义线性模型（generalized linear model）
$g(y)=\omega^Tx+b$ 或者 $y=g^{-1}(\omega^Tx+b)$,其中g为联系函数（link function）
例如：对数线性回归（log-linear regression）：$lny=\omega^Tx+b$

为了解决二分类问题，从线性回归$z=\omega^Tx+b$出发，定义“单位阶跃函数”（unit-step function）
{% math %}y=\begin{cases}
0,\quad z<0;\\
0.5,\quad	z=0;\\
1,\quad		z>0;\\
\end{cases}{% endmath %}

但是单位阶跃函数不是连续的，不可微，因此需要找一个替代函数（surrogate function）
对数几率行数（logistic function） $y=\frac{1}{1+e^{-z}}$ 符合条件并比较常用，它是一种“Sigmoid函数”,代入得到下式：
$$y=\frac{1}{1+e^{-(\omega^Tx+b)}} \quad或者\quad ln{\frac{y}{1-y}}=\omega^Tx+b$$
若将y视为样本x作为正例的可能性，则1-y则是反例可能性，两者的比值$\frac{y}{1-y}$称为“几率”（odds）,$ln{\frac{y}{1-y}}$称为“对数几率”（logit）
